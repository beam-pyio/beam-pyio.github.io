[{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Announcement","url":"/categories/announcement/"}],"content":"We are happy to present the first release of Firehose PyIO Connector.\nâœ¨NEW\nAdd a composite transform (WriteToFirehose) that puts records into a Firehose delivery stream in batch, using the put_record_batch method of the boto3 package. Create a dedicated pipeline option (FirehoseOptions) that reads AWS related values (e.g. aws_access_key_id) from pipeline arguments. Implement metric objects that record the total, succeeded and failed elements counts. Add unit and integration testing cases. The moto and localstack-utils are used for unit and integration testing respectively. Also, a custom test client is created for testing retry behavior, which is not supported by the moto package. See this post for more examples.\n","date":"July 19, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/firehose-pyio-0.1.0/","series":[],"smallImg":"","tags":[],"timestamp":1721347200,"title":"Firehose PyIO Connector 0.1.0"},{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Demonstration","url":"/categories/demonstration/"}],"content":"Amazon Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service and Amazon OpenSearch Serverless. The Apache Beam Python I/O connector for Amazon Data Firehose (firehose_pyio) provides a data sink feature that facilitates integration with those services.\nUsage The connector can be installed from PyPI.\n1pip install firehose_pyio It has the main composite transform (WriteToFirehose), and it expects a list or tuple PCollection element. If the element is a tuple, the tuple\u0026rsquo;s first element is taken. If the element is not of the accepted types, you can apply the GroupIntoBatches or BatchElements transform beforehand. Then, the element is sent into a Firehose delivery stream using the put_record_batch method of the boto3 package. Note that the above batch transforms can also be useful to overcome the API limitation listed below.\nEach PutRecordBatch request supports up to 500 records. Each record in the request can be as large as 1,000 KB (before base64 encoding), up to a limit of 4 MB for the entire request. These limits cannot be changed. The transform also has options that control individual records or handling failed records.\njsonify - A flag that indicates whether to convert a record into Json. Note that a record should be of bytes, bytearray or file-like object, and, if it is not of a supported type (e.g. integer), we can convert it into a Json string by specifying this flag to True. multiline - A flag that indicates whether to add a new line character (\\n) to each record. It is useful to save records into a CSV or Jsonline file. max_trials - The maximum number of trials when there is one or more failed records - it defaults to 3. Note that failed records after all trials are returned, which allows users to determine how to handle them subsequently. Example The example shows how to put records to a Firehose delivery stream that delivers data into an S3 bucket. We first need to create a delivery stream and related resources using the following Python script. The source can be found in the examples folder of the connector repository.\n1# examples/create_resources.py 2import json 3import boto3 4from botocore.exceptions import ClientError 5 6COMMON_NAME = \u0026#34;firehose-pyio-test\u0026#34; 7 8 9def create_destination_bucket(bucket_name): 10 client = boto3.client(\u0026#34;s3\u0026#34;) 11 suffix = client._client_config.region_name 12 client.create_bucket(Bucket=f\u0026#34;{bucket_name}-{suffix}\u0026#34;) 13 14 15def create_firehose_iam_role(role_name): 16 assume_role_policy_document = json.dumps( 17 { 18 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 19 \u0026#34;Statement\u0026#34;: [ 20 { 21 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 22 \u0026#34;Principal\u0026#34;: {\u0026#34;Service\u0026#34;: \u0026#34;firehose.amazonaws.com\u0026#34;}, 23 \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, 24 } 25 ], 26 } 27 ) 28 client = boto3.client(\u0026#34;iam\u0026#34;) 29 try: 30 return client.get_role(RoleName=role_name) 31 except ClientError as error: 32 if error.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;NoSuchEntity\u0026#34;: 33 resp = client.create_role( 34 RoleName=role_name, AssumeRolePolicyDocument=assume_role_policy_document 35 ) 36 client.attach_role_policy( 37 RoleName=role_name, 38 PolicyArn=\u0026#34;arn:aws:iam::aws:policy/AmazonS3FullAccess\u0026#34;, 39 ) 40 return resp 41 42 43def create_delivery_stream(delivery_stream_name, role_arn, bucket_name): 44 client = boto3.client(\u0026#34;firehose\u0026#34;) 45 suffix = client._client_config.region_name 46 try: 47 client.create_delivery_stream( 48 DeliveryStreamName=delivery_stream_name, 49 DeliveryStreamType=\u0026#34;DirectPut\u0026#34;, 50 S3DestinationConfiguration={ 51 \u0026#34;RoleARN\u0026#34;: role_arn, 52 \u0026#34;BucketARN\u0026#34;: f\u0026#34;arn:aws:s3:::{bucket_name}-{suffix}\u0026#34;, 53 \u0026#34;BufferingHints\u0026#34;: {\u0026#34;SizeInMBs\u0026#34;: 1, \u0026#34;IntervalInSeconds\u0026#34;: 0}, 54 }, 55 ) 56 except ClientError as error: 57 if error.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;ResourceInUseException\u0026#34;: 58 pass 59 else: 60 raise error 61 62 63if __name__ == \u0026#34;__main__\u0026#34;: 64 print(\u0026#34;create a destination bucket...\u0026#34;) 65 create_destination_bucket(COMMON_NAME) 66 print(\u0026#34;create an iam role...\u0026#34;) 67 iam_resp = create_firehose_iam_role(COMMON_NAME) 68 print(\u0026#34;create a delivery stream...\u0026#34;) 69 create_delivery_stream(COMMON_NAME, iam_resp[\u0026#34;Role\u0026#34;][\u0026#34;Arn\u0026#34;], COMMON_NAME) The main example script is constructed so that it (1) deletes all existing objects in the S3 bucket, (2) runs the example pipeline and (3) prints contents of the object(s) created by the pipeline.\nThe pipeline begins with creating sample elements where each element is a dictionary that has the id, name and created_at (created date time) attributes. Then, we apply the following two transforms before we apply the main transform (WriteToFirehose).\nDatetimeToStr - It converts the created_at attribute values into string because the Python datetime class cannot be converted into Json by default. BatchElements - It batches the elements into the minimum batch size of 50. It prevents the individual dictionary element from being pushed into the WriteToFirehose transform. In the WriteToFirehose transform, it is configured that individual records are converted into Json (jsonify=True) as well as a new line character is appended (multiline=True). The former is required because the Python dictionary is not a supported data type while the latter makes the records are saved as Jsonline.\n1# examples/pipeline.py 2import argparse 3import datetime 4import random 5import string 6import logging 7import boto3 8import time 9 10import apache_beam as beam 11from apache_beam.transforms.util import BatchElements 12from apache_beam.options.pipeline_options import PipelineOptions 13from apache_beam.options.pipeline_options import SetupOptions 14 15from firehose_pyio.io import WriteToFirehose 16 17 18def get_all_contents(bucket_name): 19 client = boto3.client(\u0026#34;s3\u0026#34;) 20 bucket_objects = client.list_objects_v2( 21 Bucket=f\u0026#34;{bucket_name}-{client._client_config.region_name}\u0026#34; 22 ) 23 return bucket_objects.get(\u0026#34;Contents\u0026#34;) or [] 24 25 26def delete_all_objects(bucket_name): 27 client = boto3.client(\u0026#34;s3\u0026#34;) 28 contents = get_all_contents(bucket_name) 29 for content in contents: 30 client.delete_object( 31 Bucket=f\u0026#34;{bucket_name}-{client._client_config.region_name}\u0026#34;, 32 Key=content[\u0026#34;Key\u0026#34;], 33 ) 34 35 36def print_bucket_contents(bucket_name): 37 client = boto3.client(\u0026#34;s3\u0026#34;) 38 contents = get_all_contents(bucket_name) 39 for content in contents: 40 resp = client.get_object( 41 Bucket=f\u0026#34;{bucket_name}-{client._client_config.region_name}\u0026#34;, 42 Key=content[\u0026#34;Key\u0026#34;], 43 ) 44 print(f\u0026#34;Key - {content[\u0026#39;Key\u0026#39;]}\u0026#34;) 45 print(resp[\u0026#34;Body\u0026#34;].read().decode()) 46 47 48def create_records(n=100): 49 return [ 50 { 51 \u0026#34;id\u0026#34;: i, 52 \u0026#34;name\u0026#34;: \u0026#34;\u0026#34;.join(random.choices(string.ascii_letters, k=5)).lower(), 53 \u0026#34;created_at\u0026#34;: datetime.datetime.now(), 54 } 55 for i in range(n) 56 ] 57 58 59def convert_ts(record: dict): 60 record[\u0026#34;created_at\u0026#34;] = record[\u0026#34;created_at\u0026#34;].isoformat(timespec=\u0026#34;milliseconds\u0026#34;) 61 return record 62 63 64def mask_secrets(d: dict): 65 return {k: (v if k.find(\u0026#34;aws\u0026#34;) \u0026lt; 0 else \u0026#34;x\u0026#34; * len(v)) for k, v in d.items()} 66 67 68def run(argv=None, save_main_session=True): 69 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 70 parser.add_argument( 71 \u0026#34;--stream_name\u0026#34;, 72 default=\u0026#34;firehose-pyio-test\u0026#34;, 73 type=str, 74 help=\u0026#34;Delivery stream name\u0026#34;, 75 ) 76 parser.add_argument( 77 \u0026#34;--num_records\u0026#34;, default=\u0026#34;100\u0026#34;, type=int, help=\u0026#34;Number of records\u0026#34; 78 ) 79 known_args, pipeline_args = parser.parse_known_args(argv) 80 81 pipeline_options = PipelineOptions(pipeline_args) 82 pipeline_options.view_as(SetupOptions).save_main_session = save_main_session 83 print(f\u0026#34;known_args - {known_args}\u0026#34;) 84 print(f\u0026#34;pipeline options - {mask_secrets(pipeline_options.display_data())}\u0026#34;) 85 86 with beam.Pipeline(options=pipeline_options) as p: 87 ( 88 p 89 | \u0026#34;CreateElements\u0026#34; \u0026gt;\u0026gt; beam.Create(create_records(known_args.num_records)) 90 | \u0026#34;DatetimeToStr\u0026#34; \u0026gt;\u0026gt; beam.Map(convert_ts) 91 | \u0026#34;BatchElements\u0026#34; \u0026gt;\u0026gt; BatchElements(min_batch_size=50) 92 | \u0026#34;WriteToFirehose\u0026#34; 93 \u0026gt;\u0026gt; WriteToFirehose( 94 delivery_stream_name=known_args.stream_name, 95 jsonify=True, 96 multiline=True, 97 max_trials=3, 98 ) 99 ) 100 101 logging.getLogger().setLevel(logging.WARN) 102 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 103 104 105if __name__ == \u0026#34;__main__\u0026#34;: 106 BUCKET_NAME = \u0026#34;firehose-pyio-test\u0026#34; 107 print(\u0026#34;\u0026gt;\u0026gt; delete existing objects...\u0026#34;) 108 delete_all_objects(BUCKET_NAME) 109 print(\u0026#34;\u0026gt;\u0026gt; start pipeline...\u0026#34;) 110 run() 111 time.sleep(1) 112 print(\u0026#34;\u0026gt;\u0026gt; print bucket contents...\u0026#34;) 113 print_bucket_contents(BUCKET_NAME) We can run the pipeline on any runner that supports the Python SDK. Below shows an example of running the example pipeline on the Apache Flink Runner. Note that AWS related values (e.g. aws_access_key_id) can be specified as pipeline arguments because the package has a dedicated pipeline option (FirehoseOptions) that parses them. Once the pipeline runs successfully, the script continues to read the contents of the file object(s) that are created by the connector.\n1python examples/pipeline.py \\ 2 --runner=FlinkRunner \\ 3 --parallelism=1 \\ 4 --aws_access_key_id=$AWS_ACCESS_KEY_ID \\ 5 --aws_secret_access_key=$AWS_SECRET_ACCESS_KEY \\ 6 --region_name=$AWS_DEFAULT_REGION 1\u0026gt;\u0026gt; start pipeline... 2known_args - Namespace(stream_name=\u0026#39;firehose-pyio-test\u0026#39;, num_records=100) 3pipeline options - {\u0026#39;runner\u0026#39;: \u0026#39;FlinkRunner\u0026#39;, \u0026#39;save_main_session\u0026#39;: True, \u0026#39;parallelism\u0026#39;: 1, \u0026#39;aws_access_key_id\u0026#39;: \u0026#39;xxxxxxxxxxxxxxxxxxxx\u0026#39;, \u0026#39;aws_secret_access_key\u0026#39;: \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;, \u0026#39;region_name\u0026#39;: \u0026#39;us-east-1\u0026#39;} 4\u0026gt;\u0026gt; print bucket contents... 5Key - 2024/07/21/01/firehose-pyio-test-1-2024-07-21-01-49-51-5fd0fc8d-b656-4f7b-9bc6-5039975d941f 6{\u0026#34;id\u0026#34;: 50, \u0026#34;name\u0026#34;: \u0026#34;amlis\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 7{\u0026#34;id\u0026#34;: 51, \u0026#34;name\u0026#34;: \u0026#34;bqvhr\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 8{\u0026#34;id\u0026#34;: 52, \u0026#34;name\u0026#34;: \u0026#34;stsbv\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 9{\u0026#34;id\u0026#34;: 53, \u0026#34;name\u0026#34;: \u0026#34;rttjg\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 10{\u0026#34;id\u0026#34;: 54, \u0026#34;name\u0026#34;: \u0026#34;avozb\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 11{\u0026#34;id\u0026#34;: 55, \u0026#34;name\u0026#34;: \u0026#34;fyesu\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 12{\u0026#34;id\u0026#34;: 56, \u0026#34;name\u0026#34;: \u0026#34;pvxlw\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 13{\u0026#34;id\u0026#34;: 57, \u0026#34;name\u0026#34;: \u0026#34;qyjlo\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 14{\u0026#34;id\u0026#34;: 58, \u0026#34;name\u0026#34;: \u0026#34;smhns\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 15... Note that the following warning messages are printed if it installs the grpcio (1.65.x) package (see this GitHub issue). You can downgrad the package version to avoid those messages (eg pip install grpcio==1.64.1).\n1WARNING: All log messages before absl::InitializeLog() is called are written to STDERR 2I0000 00:00:1721526572.886302 78332 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache 3I0000 00:00:1721526573.143589 78363 subchannel.cc:806] subchannel 0x7f4010001890 {address=ipv6:%5B::1%5D:58713, args={grpc.client_channel_factory=0x2ae79b0, grpc.default_authority=localhost:58713, grpc.internal.channel_credentials=0x297dda0, grpc.internal.client_channel_call_destination=0x7f407f3b23d0, grpc.internal.event_engine=0x7f4010013870, grpc.internal.security_connector=0x7f40100140e0, grpc.internal.subchannel_pool=0x21d3d00, grpc.max_receive_message_length=-1, grpc.max_send_message_length=-1, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x2a99310, grpc.server_uri=dns:///localhost:58713}}: connect failed (UNKNOWN:Failed to connect to remote host: connect: Connection refused (111) {created_time:\u0026#34;2024-07-21T11:49:33.143192444+10:00\u0026#34;}), backing off for 1000 ms More Examples More usage examples can be found in the unit testing cases. Some of them are covered here.\nOnly the list or tuple types are supported PCollection elements. In the following example, individual string elements are applied in the WriteToFirehose, and it raises the TypeError. 1def test_write_to_firehose_with_unsupported_types(self): 2 # only the list type is supported! 3 with self.assertRaises(TypeError): 4 with TestPipeline(options=self.pipeline_opts) as p: 5 ( 6 p 7 | beam.Create([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]) 8 | WriteToFirehose(self.delivery_stream_name, True, False) 9 ) Jsonify the element if it is not of the bytes, bytearray or file-like object. In this example, the second element is a list of integers, and it should be converted into Json (jsonify=True). Or we can convert it into string manually. 1def test_write_to_firehose_with_list_elements(self): 2 with TestPipeline(options=self.pipeline_opts) as p: 3 output = ( 4 p 5 | beam.Create([[\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;], [1, 2, 3, 4]]) 6 | WriteToFirehose(self.delivery_stream_name, True, False) 7 ) 8 assert_that(output, equal_to([])) 9 10 bucket_contents = collect_bucket_contents(self.s3_client, self.bucket_name) 11 self.assertSetEqual( 12 set(bucket_contents), set([\u0026#39;\u0026#34;one\u0026#34;\u0026#34;two\u0026#34;\u0026#34;three\u0026#34;\u0026#34;four\u0026#34;\u0026#39;, \u0026#34;1234\u0026#34;]) 13 ) If an element is a tuple, its first element is applied to the WriteToFirehose transform. 1def test_write_to_firehose_with_tuple_elements(self): 2 with TestPipeline(options=self.pipeline_opts) as p: 3 output = ( 4 p 5 | beam.Create([(1, [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]), (2, [1, 2, 3, 4])]) 6 | WriteToFirehose(self.delivery_stream_name, True, False) 7 ) 8 assert_that(output, equal_to([])) 9 10 bucket_contents = collect_bucket_contents(self.s3_client, self.bucket_name) 11 self.assertSetEqual( 12 set(bucket_contents), set([\u0026#39;\u0026#34;one\u0026#34;\u0026#34;two\u0026#34;\u0026#34;three\u0026#34;\u0026#34;four\u0026#34;\u0026#39;, \u0026#34;1234\u0026#34;]) 13 ) We can batch an element if it is not of the supported types. Note that, a new line character (\\n) is appended to each record, and it is particularly useful for saving a CSV or Jsonline file to S3. 1def test_write_to_firehose_with_list_multilining(self): 2 with TestPipeline(options=self.pipeline_opts) as p: 3 output = ( 4 p 5 | beam.Create([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]) 6 | BatchElements(min_batch_size=2, max_batch_size=2) 7 | WriteToFirehose(self.delivery_stream_name, False, True) 8 ) 9 assert_that(output, equal_to([])) 10 11 bucket_contents = collect_bucket_contents(self.s3_client, self.bucket_name) 12 self.assertSetEqual(set(bucket_contents), set([\u0026#34;one\\ntwo\\n\u0026#34;, \u0026#34;three\\nfour\\n\u0026#34;])) 13 14def test_write_to_firehose_with_tuple_multilining(self): 15 with TestPipeline(options=self.pipeline_opts) as p: 16 output = ( 17 p 18 | beam.Create([(1, \u0026#34;one\u0026#34;), (2, \u0026#34;three\u0026#34;), (1, \u0026#34;two\u0026#34;), (2, \u0026#34;four\u0026#34;)]) 19 | GroupIntoBatches(batch_size=2) 20 | WriteToFirehose(self.delivery_stream_name, False, True) 21 ) 22 assert_that(output, equal_to([])) 23 24 bucket_contents = collect_bucket_contents(self.s3_client, self.bucket_name) 25 self.assertSetEqual(set(bucket_contents), set([\u0026#34;one\\ntwo\\n\u0026#34;, \u0026#34;three\\nfour\\n\u0026#34;])) Failed records after all trials are returned. The following example configures only a single record is saved successfully into a delivery stream in each trial for 3 times. After all trials, the failed record (four) is returned from the WriteToFirehose transform. It is up to the user how to handle failed records. 1def test_write_to_firehose_retry_with_failed_elements(self): 2 with TestPipeline() as p: 3 output = ( 4 p 5 | beam.Create([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]) 6 | BatchElements(min_batch_size=4) 7 | WriteToFirehose( 8 \u0026#34;non-existing-delivery-stream\u0026#34;, False, False, 3, {\u0026#34;num_success\u0026#34;: 1} 9 ) 10 ) 11 assert_that(output, equal_to([\u0026#34;four\u0026#34;])) ","date":"July 18, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/firehose-pyio-intro/","series":[],"smallImg":"","tags":[],"timestamp":1721260800,"title":"Introduction to Firehose PyIO Connector"},{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Announcement","url":"/categories/announcement/"}],"content":"Apache Beam is an open source unified programming model to define and execute data processing pipelines, including ETL, batch and stream processing. We consider it has a huge potential to improve traditional development patterns in both transactional and analytical processing of data. Specifically it can be applied to event-driven applications, data pipelines and streaming analytics.\nEmploying dataflow programming, Beam supports a range of I/O connectors, but we find some gaps in the existing connectors especially in relation to the Python SDK. It fueled us to start the Apache Beam Python I/O Connectors project.\nAs long time AWS users, we see connectors for some key AWS services are missing or are not available in the Python SDK. Those services cover Firehose, SQS, SNS, DynamoDB and EventBridge, and we have started developing connectors for them.\nIn data engineering projects, OLTP/OLAP systems and open table formats (Apache Iceberg, Apache Hudi and Delta Lake) are key data sources and destinations (sinks). Python-native connectors can make it simpler to develop data pipelines that deals with those data storage systems, and we plan to develop relevant connectors by integrating with the daft package.\nWe keep looking into adding new connectors. If you have a new idea, please add a comment to this discussion or check the project repository for updates on new connectors.\n","date":"July 4, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/beam-pyio-intro/","series":[],"smallImg":"","tags":[],"timestamp":1720051200,"title":"Apache Beam Python I/O Connectors"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"}]
