[{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Announcement","url":"/categories/announcement/"}],"content":"We are happy to present the first release of the Apache Beam Python I/O connector for Amazon DynamoDB.\n✨NEW\nAdd a composite transform (WriteToDynamoDB) that writes records to a DynamoDB table with help of the batch_writer of the boto3 package. The batch writer will automatically handle buffering and sending items in batches. In addition, it will also automatically handle any unprocessed items and resend them as needed. Provide an option that handles duplicate records dedup_pkeys - List of keys to be used for deduplicating items in buffer. Create a dedicated pipeline option (DynamoDBOptions) that reads AWS related values (e.g. aws_access_key_id) from pipeline arguments. Implement a metric object that records the total counts. Add unit and integration testing cases. The moto and localstack-utils are used for unit and integration testing respectively. Integrate with GitHub Actions by adding workflows for testing, documentation and release management. See Introduction to DynamoDB PyIO Sink Connector for more examples.\n","date":"September 24, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/dynamodb-pyio-0.1.0/","series":[],"smallImg":"","tags":[],"timestamp":1727136000,"title":"DynamoDB PyIO Connector 0.1.0"},{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Demonstration","url":"/categories/demonstration/"}],"content":"Amazon DynamoDB is a serverless, NoSQL database service that allows you to develop modern applications at any scale. The Apache Beam Python I/O connector for Amazon DynamoDB (dynamodb_pyio) aims to integrate with the database service by supporting source and sink connectors. Currently, the sink connector is available.\nInstallation The connector can be installed from PyPI.\n1pip install dynamodb_pyio Usage Sink Connector It has the main composite transform (WriteToDynamoDB), and it expects a list or tuple PCollection element. If the element is a tuple, the tuple\u0026rsquo;s first element is taken. If the element is not of the accepted types, you can apply the GroupIntoBatches or BatchElements transform beforehand. Then, the records of the element are written to a DynamoDB table with help of the batch_writer of the boto3 package. Note that the batch writer will automatically handle buffering and sending items in batches. In addition, it will also automatically handle any unprocessed items and resend them as needed.\nThe transform also has an option that handles duplicate records - see the example below for more details.\ndedup_pkeys - List of keys to be used for deduplicating items in buffer. Sink Connector Example The example shows how to write records in batch to a DynamoDB table using the sink connector. The source can be found in the examples folder of the connector repository.\nThe pipeline begins with creating a configurable number records (default 500) where each record has two attributes (pk and sk). The attribute values are configured so that the first half of the records have incremental values while the remaining half have a single value of 1. Therefore, as the attributes are the hash and sort key of the table, we can expect only 250 records in the DynamoDB table if we configure 500 records. Moreover, in spite of duplicate values, we do not encounter an error if we specify the dedup_pkeys value correctly.\nAfter creating elements, we apply the BatchElements transform where the minimum and maximum batch sizes are set to 100 and 200 respectively. Note that it prevents individual dictionary elements from being pushed into the WriteToDynamoDB transform. Finally, batches of elements are written to the DynamoDB table using the WriteToDynamoDB transform.\n1# pipeline.py 2import argparse 3import decimal 4import logging 5 6import boto3 7from boto3.dynamodb.types import TypeDeserializer 8 9import apache_beam as beam 10from apache_beam.transforms.util import BatchElements 11from apache_beam.options.pipeline_options import PipelineOptions 12from apache_beam.options.pipeline_options import SetupOptions 13 14from dynamodb_pyio.io import WriteToDynamoDB 15 16TABLE_NAME = \u0026#34;dynamodb-pyio-test\u0026#34; 17 18 19def get_table(table_name): 20 resource = boto3.resource(\u0026#34;dynamodb\u0026#34;) 21 return resource.Table(table_name) 22 23 24def create_table(table_name): 25 client = boto3.client(\u0026#34;dynamodb\u0026#34;) 26 try: 27 client.describe_table(TableName=table_name) 28 table_exists = True 29 except Exception: 30 table_exists = False 31 if not table_exists: 32 print(\u0026#34;\u0026gt;\u0026gt; create table...\u0026#34;) 33 params = { 34 \u0026#34;TableName\u0026#34;: table_name, 35 \u0026#34;KeySchema\u0026#34;: [ 36 {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34;}, 37 {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34;}, 38 ], 39 \u0026#34;AttributeDefinitions\u0026#34;: [ 40 {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, 41 {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;N\u0026#34;}, 42 ], 43 \u0026#34;BillingMode\u0026#34;: \u0026#34;PAY_PER_REQUEST\u0026#34;, 44 } 45 client.create_table(**params) 46 get_table(table_name).wait_until_exists() 47 48 49def to_int_if_decimal(v): 50 try: 51 if isinstance(v, decimal.Decimal): 52 return int(v) 53 else: 54 return v 55 except Exception: 56 return v 57 58 59def scan_table(**kwargs): 60 client = boto3.client(\u0026#34;dynamodb\u0026#34;) 61 paginator = client.get_paginator(\u0026#34;scan\u0026#34;) 62 page_iterator = paginator.paginate(**kwargs) 63 items = [] 64 for page in page_iterator: 65 for document in page[\u0026#34;Items\u0026#34;]: 66 items.append( 67 { 68 k: to_int_if_decimal(TypeDeserializer().deserialize(v)) 69 for k, v in document.items() 70 } 71 ) 72 return sorted(items, key=lambda d: d[\u0026#34;sk\u0026#34;]) 73 74 75def truncate_table(table_name): 76 records = scan_table(TableName=TABLE_NAME) 77 table = get_table(table_name) 78 with table.batch_writer() as batch: 79 for record in records: 80 batch.delete_item(Key=record) 81 82 83def mask_secrets(d: dict): 84 return {k: (v if k.find(\u0026#34;aws\u0026#34;) \u0026lt; 0 else \u0026#34;x\u0026#34; * len(v)) for k, v in d.items()} 85 86 87def run(argv=None, save_main_session=True): 88 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 89 parser.add_argument( 90 \u0026#34;--table_name\u0026#34;, default=TABLE_NAME, type=str, help=\u0026#34;DynamoDB table name\u0026#34; 91 ) 92 parser.add_argument( 93 \u0026#34;--num_records\u0026#34;, default=\u0026#34;500\u0026#34;, type=int, help=\u0026#34;Number of records\u0026#34; 94 ) 95 known_args, pipeline_args = parser.parse_known_args(argv) 96 97 pipeline_options = PipelineOptions(pipeline_args) 98 pipeline_options.view_as(SetupOptions).save_main_session = save_main_session 99 print(f\u0026#34;known_args - {known_args}\u0026#34;) 100 print(f\u0026#34;pipeline options - {mask_secrets(pipeline_options.display_data())}\u0026#34;) 101 102 with beam.Pipeline(options=pipeline_options) as p: 103 ( 104 p 105 | \u0026#34;CreateElements\u0026#34; 106 \u0026gt;\u0026gt; beam.Create( 107 [ 108 { 109 \u0026#34;pk\u0026#34;: str(int(1 if i \u0026gt;= known_args.num_records / 2 else i)), 110 \u0026#34;sk\u0026#34;: int(1 if i \u0026gt;= known_args.num_records / 2 else i), 111 } 112 for i in range(known_args.num_records) 113 ] 114 ) 115 | \u0026#34;BatchElements\u0026#34; \u0026gt;\u0026gt; BatchElements(min_batch_size=100, max_batch_size=200) 116 | \u0026#34;WriteToDynamoDB\u0026#34; 117 \u0026gt;\u0026gt; WriteToDynamoDB( 118 table_name=known_args.table_name, dedup_pkeys=[\u0026#34;pk\u0026#34;, \u0026#34;sk\u0026#34;] 119 ) 120 ) 121 122 logging.getLogger().setLevel(logging.INFO) 123 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 124 125 126if __name__ == \u0026#34;__main__\u0026#34;: 127 create_table(TABLE_NAME) 128 print(\u0026#34;\u0026gt;\u0026gt; start pipeline...\u0026#34;) 129 run() 130 print(\u0026#34;\u0026gt;\u0026gt; check number of records...\u0026#34;) 131 print(len(scan_table(TableName=TABLE_NAME))) 132 print(\u0026#34;\u0026gt;\u0026gt; truncate table...\u0026#34;) 133 truncate_table(TABLE_NAME) We can run the pipeline on any runner that supports the Python SDK. Below shows an example of running the example pipeline on the Apache Flink Runner. Note that AWS related values (e.g. aws_access_key_id) can be specified as pipeline arguments because the package has a dedicated pipeline option (DynamoDBOptions) that parses them. Once the pipeline runs successfully, the script checks the number of records that are created by the connector followed by truncating the table.\n1python examples/pipeline.py \\ 2 --runner=FlinkRunner \\ 3 --parallelism=1 \\ 4 --aws_access_key_id=$AWS_ACCESS_KEY_ID \\ 5 --aws_secret_access_key=$AWS_SECRET_ACCESS_KEY \\ 6 --region_name=$AWS_DEFAULT_REGION 1\u0026gt;\u0026gt; create table... 2\u0026gt;\u0026gt; start pipeline... 3known_args - Namespace(table_name=\u0026#39;dynamodb-pyio-test\u0026#39;, num_records=500) 4pipeline options - {\u0026#39;runner\u0026#39;: \u0026#39;FlinkRunner\u0026#39;, \u0026#39;save_main_session\u0026#39;: True, \u0026#39;parallelism\u0026#39;: 1, \u0026#39;aws_access_key_id\u0026#39;: \u0026#39;xxxxxxxxxxxxxxxxxxxx\u0026#39;, \u0026#39;aws_secret_access_key\u0026#39;: \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;, \u0026#39;region_name\u0026#39;: \u0026#39;us-east-1\u0026#39;} 5... 6INFO:root:total 100 elements processed... 7INFO:root:total 100 elements processed... 8INFO:root:total 200 elements processed... 9INFO:root:total 100 elements processed... 10INFO:root:BatchElements statistics: element_count=500 batch_count=4 next_batch_size=101 timings=[(100, 0.9067182540893555), (200, 1.815131664276123), (100, 0.22190475463867188)] 11... 12\u0026gt;\u0026gt; check number of records... 13... 14250 15\u0026gt;\u0026gt; truncate table... Note that the following warning messages are printed if it installs the grpcio (1.65.x) package (see this GitHub issue). You can downgrade the package version to avoid those messages (e.g. pip install grpcio==1.64.1).\n1WARNING: All log messages before absl::InitializeLog() is called are written to STDERR 2I0000 00:00:1721526572.886302 78332 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache 3I0000 00:00:1721526573.143589 78363 subchannel.cc:806] subchannel 0x7f4010001890 {address=ipv6:%5B::1%5D:58713, args={grpc.client_channel_factory=0x2ae79b0, grpc.default_authority=localhost:58713, grpc.internal.channel_credentials=0x297dda0, grpc.internal.client_channel_call_destination=0x7f407f3b23d0, grpc.internal.event_engine=0x7f4010013870, grpc.internal.security_connector=0x7f40100140e0, grpc.internal.subchannel_pool=0x21d3d00, grpc.max_receive_message_length=-1, grpc.max_send_message_length=-1, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x2a99310, grpc.server_uri=dns:///localhost:58713}}: connect failed (UNKNOWN:Failed to connect to remote host: connect: Connection refused (111) {created_time:\u0026#34;2024-07-21T11:49:33.143192444+10:00\u0026#34;}), backing off for 1000 ms More Sink Connector Examples More usage examples can be found in the unit testing cases. Some of them are covered here.\nThe transform can process many records, thanks to the batch writer. 1def test_write_to_dynamodb_with_large_items(self): 2 # batch writer automatically handles buffering and sending items in batches 3 records = [{\u0026#34;pk\u0026#34;: str(i), \u0026#34;sk\u0026#34;: i} for i in range(5000)] 4 with TestPipeline(options=self.pipeline_opts) as p: 5 (p | beam.Create([records]) | WriteToDynamoDB(table_name=self.table_name)) 6 self.assertListEqual(records, scan_table(TableName=self.table_name)) Only the list or tuple types are supported PCollection elements. In the following example, individual dictionary elements are applied in the WriteToDynamoDB, and it raises the DynamoDBClientError. 1def test_write_to_dynamodb_with_unsupported_record_type(self): 2 # supported types are list or tuple where the second element is a list! 3 records = [{\u0026#34;pk\u0026#34;: str(i), \u0026#34;sk\u0026#34;: i} for i in range(20)] 4 with self.assertRaises(DynamoDBClientError): 5 with TestPipeline(options=self.pipeline_opts) as p: 6 (p | beam.Create(records) | WriteToDynamoDB(table_name=self.table_name)) Incorrect key attribute data types throws an error. 1def test_write_to_dynamodb_with_wrong_data_type(self): 2 # pk and sk should be string and number respectively 3 records = [{\u0026#34;pk\u0026#34;: i, \u0026#34;sk\u0026#34;: str(i)} for i in range(20)] 4 with self.assertRaises(DynamoDBClientError): 5 with TestPipeline(options=self.pipeline_opts) as p: 6 ( 7 p 8 | beam.Create([records]) 9 | WriteToDynamoDB(table_name=self.table_name) 10 ) Duplicate records are not allowed unless the dedup_pkeys value is correctly specified. 1def test_write_to_dynamodb_duplicate_records_without_dedup_keys(self): 2 records = [{\u0026#34;pk\u0026#34;: str(1), \u0026#34;sk\u0026#34;: 1} for _ in range(20)] 3 with self.assertRaises(DynamoDBClientError): 4 with TestPipeline(options=self.pipeline_opts) as p: 5 ( 6 p 7 | beam.Create([records]) 8 | WriteToDynamoDB(table_name=self.table_name) 9 ) 10 11def test_write_to_dynamodb_duplicate_records_with_dedup_keys(self): 12 records = [{\u0026#34;pk\u0026#34;: str(1), \u0026#34;sk\u0026#34;: 1} for _ in range(20)] 13 with TestPipeline(options=self.pipeline_opts) as p: 14 ( 15 p 16 | beam.Create([records]) 17 | WriteToDynamoDB(table_name=self.table_name, dedup_pkeys=[\u0026#34;pk\u0026#34;, \u0026#34;sk\u0026#34;]) 18 ) 19 self.assertListEqual(records[:1], scan_table(TableName=self.table_name)) We can control batches of elements further with the BatchElements or GroupIntoBatches transform. 1def test_write_to_dynamodb_with_batch_elements(self): 2 records = [{\u0026#34;pk\u0026#34;: str(i), \u0026#34;sk\u0026#34;: i} for i in range(20)] 3 with TestPipeline(options=self.pipeline_opts) as p: 4 ( 5 p 6 | beam.Create(records) 7 | BatchElements(min_batch_size=10, max_batch_size=10) 8 | WriteToDynamoDB(table_name=self.table_name) 9 ) 10 self.assertListEqual(records, scan_table(TableName=self.table_name)) 11 12def test_write_to_dynamodb_with_group_into_batches(self): 13 records = [(i % 2, {\u0026#34;pk\u0026#34;: str(i), \u0026#34;sk\u0026#34;: i}) for i in range(20)] 14 with TestPipeline(options=self.pipeline_opts) as p: 15 ( 16 p 17 | beam.Create(records) 18 | GroupIntoBatches(batch_size=10) 19 | WriteToDynamoDB(table_name=self.table_name) 20 ) 21 self.assertListEqual( 22 [r[1] for r in records], scan_table(TableName=self.table_name) 23 ) ","date":"September 23, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/dynamodb-pyio-intro/","series":[],"smallImg":"","tags":[],"timestamp":1727049600,"title":"Introduction to DynamoDB PyIO Sink Connector"},{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Announcement","url":"/categories/announcement/"}],"content":"We are happy to present the Apache Beam Python I/O connector for Amazon Data Firehose 0.2.1 release.\n♻️UPDATE\nReturn failed elements by a tagged output, which allows users to determine how to handle them subsequently. Provide options that handle failed records. max_trials - The maximum number of trials when there is one or more failed records. append_error - Whether to append error details to failed records. See Introduction to Firehose PyIO Sink Connector for more examples.\n","date":"August 22, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/firehose-pyio-0.2.1/","series":[],"smallImg":"","tags":[],"timestamp":1724284800,"title":"Firehose PyIO Connector 0.2.1"},{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Announcement","url":"/categories/announcement/"}],"content":"We are happy to present the first release of the Apache Beam Python I/O connector for Amazon SQS.\n✨NEW\nAdd a composite transform (WriteToSqs) that sends messages to a SQS queue in batch, using the send_message_batch method of the boto3 package. Provide options that handle failed records. max_trials - The maximum number of trials when there is one or more failed records. append_error - Whether to append error details to failed records. Return failed elements by a tagged output, which allows users to determine how to handle them subsequently. Create a dedicated pipeline option (SqsOptions) that reads AWS related values (e.g. aws_access_key_id) from pipeline arguments. Implement metric objects that record the total, succeeded and failed elements counts. Add unit and integration testing cases. The moto and localstack-utils are used for unit and integration testing respectively. Also, a custom test client is created for testing retry behavior, which is not supported by the moto package. Integrate with GitHub Actions by adding workflows for testing, documentation and release management. See Introduction to SQS PyIO Sink Connector for more examples.\n","date":"August 20, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/sqs-pyio-0.1.0/","series":[],"smallImg":"","tags":[],"timestamp":1724112000,"title":"SQS PyIO Connector 0.1.0"},{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Demonstration","url":"/categories/demonstration/"}],"content":"Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. The Apache Beam Python I/O connector for Amazon SQS (sqs_pyio) aims to integrate with the queue service by supporting source and sink connectors. Currently, the sink connector is available.\nInstallation The connector can be installed from PyPI.\n1pip install sqs_pyio Usage Sink Connector It has the main composite transform (WriteToSqs), and it expects a list or tuple PCollection element. If the element is a tuple, the tuple\u0026rsquo;s first element is taken. If the element is not of the accepted types, you can apply the GroupIntoBatches or BatchElements transform beforehand. Then, the records of the element are sent into a SQS queue using the send_message_batch method of the boto3 package. Note that the above batch transforms can also be useful to overcome the API limitation listed below.\nEach SendMessageBatch request supports up to 10 messages. The maximum allowed individual message size and the maximum total payload size (the sum of the individual lengths of all the batched messages) are both 256 KiB (262,144 bytes). The transform also has options that handle failed records as listed below.\nmax_trials - The maximum number of trials when there is one or more failed records - it defaults to 3. Note that failed records after all trials are returned by a tagged output, which allows users to determine how to handle them subsequently. append_error - Whether to append error details to failed records. Defaults to True. As mentioned earlier, failed elements are returned by a tagged output where it is named as write-to-sqs-failed-output by default. You can change the name by specifying a different name using the failed_output argument.\nSink Connector Example The example shows how to send messages in batch to a SQS queue using the sink connector and check the approximate number of messages in the queue. The source can be found in the examples folder of the connector repository.\nThe pipeline begins with creating sample elements where each element is a dictionary that has the Id and MessageBody attributes. Then, we apply the BatchElements transform where the minimum and maximum batch sizes are set to 10. Note that it prevents the individual dictionary element from being pushed into the WriteToSqs transform as well as it allows us to bypass the API limitation. Finally, in the WriteToSqs transform, it is configured that a maximum of three trials are made when there are failed elements (max_trials=3) and error details are appended to failed elements (append_error=True).\n1# pipeline.py 2import argparse 3import time 4import logging 5 6import boto3 7from botocore.exceptions import ClientError 8 9import apache_beam as beam 10from apache_beam.transforms.util import BatchElements 11from apache_beam.options.pipeline_options import PipelineOptions 12from apache_beam.options.pipeline_options import SetupOptions 13 14from sqs_pyio.io import WriteToSqs 15 16QUEUE_NAME = \u0026#34;sqs-pyio-test\u0026#34; 17 18 19def get_queue_url(queue_name): 20 client = boto3.client(\u0026#34;sqs\u0026#34;) 21 try: 22 return client.get_queue_url(QueueName=queue_name)[\u0026#34;QueueUrl\u0026#34;] 23 except ClientError as error: 24 if error.response[\u0026#34;Error\u0026#34;][\u0026#34;QueryErrorCode\u0026#34;] == \u0026#34;QueueDoesNotExist\u0026#34;: 25 client.create_queue(QueueName=queue_name) 26 return client.get_queue_url(QueueName=queue_name)[\u0026#34;QueueUrl\u0026#34;] 27 else: 28 raise error 29 30 31def purge_queue(queue_name): 32 client = boto3.client(\u0026#34;sqs\u0026#34;) 33 queue_url = get_queue_url(queue_name) 34 try: 35 client.purge_queue(QueueUrl=queue_url) 36 except ClientError as error: 37 if error.response[\u0026#34;Error\u0026#34;][\u0026#34;QueryErrorCode\u0026#34;] != \u0026#34;PurgeQueueInProgress\u0026#34;: 38 raise error 39 40 41def check_number_of_messages(queue_name): 42 client = boto3.client(\u0026#34;sqs\u0026#34;) 43 queue_url = get_queue_url(queue_name) 44 resp = client.get_queue_attributes( 45 QueueUrl=queue_url, AttributeNames=[\u0026#34;ApproximateNumberOfMessages\u0026#34;] 46 ) 47 return f\u0026#39;{resp[\u0026#34;Attributes\u0026#34;][\u0026#34;ApproximateNumberOfMessages\u0026#34;]} messages are found approximately!\u0026#39; 48 49 50def mask_secrets(d: dict): 51 return {k: (v if k.find(\u0026#34;aws\u0026#34;) \u0026lt; 0 else \u0026#34;x\u0026#34; * len(v)) for k, v in d.items()} 52 53 54def run(argv=None, save_main_session=True): 55 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 56 parser.add_argument( 57 \u0026#34;--queue_name\u0026#34;, default=QUEUE_NAME, type=str, help=\u0026#34;SQS queue name\u0026#34; 58 ) 59 parser.add_argument( 60 \u0026#34;--num_records\u0026#34;, default=\u0026#34;100\u0026#34;, type=int, help=\u0026#34;Number of records\u0026#34; 61 ) 62 known_args, pipeline_args = parser.parse_known_args(argv) 63 64 pipeline_options = PipelineOptions(pipeline_args) 65 pipeline_options.view_as(SetupOptions).save_main_session = save_main_session 66 print(f\u0026#34;known_args - {known_args}\u0026#34;) 67 print(f\u0026#34;pipeline options - {mask_secrets(pipeline_options.display_data())}\u0026#34;) 68 69 with beam.Pipeline(options=pipeline_options) as p: 70 ( 71 p 72 | \u0026#34;CreateElements\u0026#34; 73 \u0026gt;\u0026gt; beam.Create( 74 [ 75 {\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: str(i)} 76 for i in range(known_args.num_records) 77 ] 78 ) 79 | \u0026#34;BatchElements\u0026#34; \u0026gt;\u0026gt; BatchElements(min_batch_size=10, max_batch_size=10) 80 | \u0026#34;WriteToSqs\u0026#34; 81 \u0026gt;\u0026gt; WriteToSqs( 82 queue_name=known_args.queue_name, 83 max_trials=3, 84 append_error=True, 85 failed_output=\u0026#34;my-failed-output\u0026#34;, 86 ) 87 ) 88 89 logging.getLogger().setLevel(logging.INFO) 90 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 91 92 93if __name__ == \u0026#34;__main__\u0026#34;: 94 # check if queue exists 95 get_queue_url(QUEUE_NAME) 96 print(\u0026#34;\u0026gt;\u0026gt; start pipeline...\u0026#34;) 97 run() 98 time.sleep(1) 99 print(check_number_of_messages(QUEUE_NAME)) 100 print(\u0026#34;\u0026gt;\u0026gt; purge existing messages...\u0026#34;) 101 purge_queue(QUEUE_NAME) We can run the pipeline on any runner that supports the Python SDK. Below shows an example of running the example pipeline on the Apache Flink Runner. Note that AWS related values (e.g. aws_access_key_id) can be specified as pipeline arguments because the package has a dedicated pipeline option (SqsOptions) that parses them. Once the pipeline runs successfully, the script checks the approximate number of messages that are created by the connector.\n1python examples/pipeline.py \\ 2 --runner=FlinkRunner \\ 3 --parallelism=1 \\ 4 --aws_access_key_id=$AWS_ACCESS_KEY_ID \\ 5 --aws_secret_access_key=$AWS_SECRET_ACCESS_KEY \\ 6 --region_name=$AWS_DEFAULT_REGION 1\u0026gt;\u0026gt; start pipeline... 2known_args - Namespace(queue_name=\u0026#39;sqs-pyio-test\u0026#39;, num_records=100) 3pipeline options - {\u0026#39;runner\u0026#39;: \u0026#39;FlinkRunner\u0026#39;, \u0026#39;save_main_session\u0026#39;: True, \u0026#39;parallelism\u0026#39;: 1, \u0026#39;aws_access_key_id\u0026#39;: \u0026#39;xxxxxxxxxxxxxxxxxxxx\u0026#39;, \u0026#39;aws_secret_access_key\u0026#39;: \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;, \u0026#39;region_name\u0026#39;: \u0026#39;us-east-1\u0026#39;} 4... 5INFO:root:total 10, succeeded 10, failed 0... 6INFO:root:total 10, succeeded 10, failed 0... 7INFO:root:total 10, succeeded 10, failed 0... 8INFO:root:total 10, succeeded 10, failed 0... 9INFO:root:total 10, succeeded 10, failed 0... 10INFO:root:total 10, succeeded 10, failed 0... 11INFO:root:total 10, succeeded 10, failed 0... 12INFO:root:total 10, succeeded 10, failed 0... 13INFO:root:total 10, succeeded 10, failed 0... 14INFO:root:total 10, succeeded 10, failed 0... 15INFO:root:BatchElements statistics: element_count=100 batch_count=10 next_batch_size=10 timings=[(10, 0.44828295707702637), (10, 0.4457244873046875), (10, 0.4468190670013428), (10, 0.4548039436340332), (10, 0.4473147392272949), (10, 0.4668114185333252), (10, 0.4462318420410156), (10, 0.4522392749786377), (10, 0.44727039337158203)] 16... 17100 messages are found approximately! 18\u0026gt;\u0026gt; purge existing messages... Note that the following warning messages are printed if it installs the grpcio (1.65.x) package (see this GitHub issue). You can downgrade the package version to avoid those messages (e.g. pip install grpcio==1.64.1).\n1WARNING: All log messages before absl::InitializeLog() is called are written to STDERR 2I0000 00:00:1721526572.886302 78332 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache 3I0000 00:00:1721526573.143589 78363 subchannel.cc:806] subchannel 0x7f4010001890 {address=ipv6:%5B::1%5D:58713, args={grpc.client_channel_factory=0x2ae79b0, grpc.default_authority=localhost:58713, grpc.internal.channel_credentials=0x297dda0, grpc.internal.client_channel_call_destination=0x7f407f3b23d0, grpc.internal.event_engine=0x7f4010013870, grpc.internal.security_connector=0x7f40100140e0, grpc.internal.subchannel_pool=0x21d3d00, grpc.max_receive_message_length=-1, grpc.max_send_message_length=-1, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x2a99310, grpc.server_uri=dns:///localhost:58713}}: connect failed (UNKNOWN:Failed to connect to remote host: connect: Connection refused (111) {created_time:\u0026#34;2024-07-21T11:49:33.143192444+10:00\u0026#34;}), backing off for 1000 ms More Sink Connector Examples More usage examples can be found in the unit testing cases. Some of them are covered here.\nOnly the list or tuple types are supported PCollection elements. In the following example, individual dictionary elements are applied in the WriteToSqs, and it raises the SqsClientError. 1def test_write_to_sqs_with_unsupported_record_type(self): 2 # only the list type is supported! 3 records = [{\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: str(i)} for i in range(3)] 4 with self.assertRaises(SqsClientError): 5 with TestPipeline(options=self.pipeline_opts) as p: 6 (p | beam.Create(records) | WriteToSqs(queue_name=self.queue_name)) Both the list and tuple types are supported. Note the main output is tagged as None, and we can check the elements by specifying the tag value (i.e. output[None]). 1def test_write_to_sqs_with_list_element(self): 2 records = [{\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: str(i)} for i in range(3)] 3 with TestPipeline(options=self.pipeline_opts) as p: 4 output = p | beam.Create([records]) | WriteToSqs(queue_name=self.queue_name) 5 assert_that(output[None], equal_to([])) 6 7def test_write_to_sqs_with_tuple_element(self): 8 records = [{\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: str(i)} for i in range(3)] 9 with TestPipeline(options=self.pipeline_opts) as p: 10 output = ( 11 p 12 | beam.Create([(\u0026#34;key\u0026#34;, records)]) 13 | WriteToSqs(queue_name=self.queue_name) 14 ) 15 assert_that(output[None], equal_to([])) The Id and MessageBody attributes are mandatory, and they should be the string type. 1def test_write_to_sqs_with_incorrect_message_data_type(self): 2 # Id should be string 3 records = [{\u0026#34;Id\u0026#34;: i, \u0026#34;MessageBody\u0026#34;: str(i)} for i in range(3)] 4 with self.assertRaises(SqsClientError): 5 with TestPipeline(options=self.pipeline_opts) as p: 6 (p | beam.Create([records]) | WriteToSqs(queue_name=self.queue_name)) 7 8 # MessageBody should be string 9 records = [{\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: i} for i in range(3)] 10 with self.assertRaises(SqsClientError): 11 with TestPipeline(options=self.pipeline_opts) as p: 12 (p | beam.Create([records]) | WriteToSqs(queue_name=self.queue_name)) We can batch the elements with the BatchElements or GroupIntoBatches transform. 1def test_write_to_sqs_with_batch_elements(self): 2 # BatchElements groups unkeyed elements into a list 3 records = [{\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: str(i)} for i in range(3)] 4 with TestPipeline(options=self.pipeline_opts) as p: 5 output = ( 6 p 7 | beam.Create(records) 8 | BatchElements(min_batch_size=2, max_batch_size=2) 9 | WriteToSqs(queue_name=self.queue_name) 10 ) 11 assert_that(output[None], equal_to([])) 12 13def test_write_to_sqs_with_group_into_batches(self): 14 # GroupIntoBatches groups keyed elements into a list 15 records = [(i % 2, {\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: str(i)}) for i in range(3)] 16 with TestPipeline(options=self.pipeline_opts) as p: 17 output = ( 18 p 19 | beam.Create(records) 20 | GroupIntoBatches(batch_size=2) 21 | WriteToSqs(queue_name=self.queue_name) 22 ) 23 assert_that(output[None], equal_to([])) Failed records after all trials are returned by a tagged output. We can configure the number of trials (max_trials) and whether to append error details (append_error). 1class TestRetryLogic(unittest.TestCase): 2 # default failed output name 3 failed_output = \u0026#34;write-to-sqs-failed-output\u0026#34; 4 5 def test_write_to_sqs_retry_no_failed_element(self): 6 records = [{\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: str(i)} for i in range(4)] 7 with TestPipeline() as p: 8 output = ( 9 p 10 | beam.Create(records) 11 | BatchElements(min_batch_size=4) 12 | WriteToSqs( 13 queue_name=\u0026#34;test-sqs-queue\u0026#34;, 14 max_trials=3, 15 append_error=True, 16 fake_config={\u0026#34;num_success\u0026#34;: 2}, 17 ) 18 ) 19 assert_that(output[None], equal_to([])) 20 21 def test_write_to_sqs_retry_failed_element_without_appending_error(self): 22 records = [{\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: str(i)} for i in range(4)] 23 with TestPipeline() as p: 24 output = ( 25 p 26 | beam.Create(records) 27 | BatchElements(min_batch_size=4) 28 | WriteToSqs( 29 queue_name=\u0026#34;test-sqs-queue\u0026#34;, 30 max_trials=3, 31 append_error=False, 32 fake_config={\u0026#34;num_success\u0026#34;: 1}, 33 ) 34 ) 35 36 assert_that( 37 output[self.failed_output], 38 equal_to([{\u0026#34;Id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;MessageBody\u0026#34;: \u0026#34;3\u0026#34;}]), 39 ) 40 41 def test_write_to_sqs_retry_failed_element_with_appending_error(self): 42 records = [{\u0026#34;Id\u0026#34;: str(i), \u0026#34;MessageBody\u0026#34;: str(i)} for i in range(4)] 43 with TestPipeline() as p: 44 output = ( 45 p 46 | beam.Create(records) 47 | BatchElements(min_batch_size=4) 48 | WriteToSqs( 49 queue_name=\u0026#34;test-sqs-queue\u0026#34;, 50 max_trials=3, 51 append_error=True, 52 fake_config={\u0026#34;num_success\u0026#34;: 1}, 53 ) 54 ) 55 assert_that( 56 output[self.failed_output], 57 equal_to( 58 [ 59 ( 60 {\u0026#34;Id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;MessageBody\u0026#34;: \u0026#34;3\u0026#34;}, 61 { 62 \u0026#34;Id\u0026#34;: \u0026#34;3\u0026#34;, 63 \u0026#34;SenderFault\u0026#34;: False, 64 \u0026#34;Code\u0026#34;: \u0026#34;error-code\u0026#34;, 65 \u0026#34;Message\u0026#34;: \u0026#34;error-message\u0026#34;, 66 }, 67 ), 68 ] 69 ), 70 ) ","date":"August 19, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/sqs-pyio-intro/","series":[],"smallImg":"","tags":[],"timestamp":1724025600,"title":"Introduction to SQS PyIO Sink Connector"},{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Announcement","url":"/categories/announcement/"}],"content":"We are happy to present the first release of the Apache Beam Python I/O connector for Amazon Data Firehose.\n✨NEW\nAdd a composite transform (WriteToFirehose) that puts records into a Firehose delivery stream in batch, using the put_record_batch method of the boto3 package. Provide options that control individual records. jsonify - A flag that indicates whether to convert a record into JSON. Note that a record should be of bytes, bytearray or file-like object, and, if it is not of a supported type (e.g. integer), we can convert it into a Json string by specifying this flag to True. multiline - A flag that indicates whether to add a new line character (\\n) to each record. It is useful to save records into a CSV or Jsonline file. Create a dedicated pipeline option (FirehoseOptions) that reads AWS related values (e.g. aws_access_key_id) from pipeline arguments. Implement metric objects that record the total, succeeded and failed elements counts. Add unit and integration testing cases. The moto and localstack-utils packages are used for unit and integration testing respectively. Also, a custom test client is created for testing retry of failed elements, which is not supported by the moto package. See Introduction to Firehose PyIO Sink Connector for more examples.\n","date":"July 19, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/firehose-pyio-0.1.0/","series":[],"smallImg":"","tags":[],"timestamp":1721347200,"title":"Firehose PyIO Connector 0.1.0"},{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Demonstration","url":"/categories/demonstration/"}],"content":"Amazon Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service and Amazon OpenSearch Serverless. The Apache Beam Python I/O connector for Amazon Data Firehose (firehose_pyio) provides a data sink feature that facilitates integration with those services.\nInstallation The connector can be installed from PyPI.\n1pip install firehose_pyio Usage Sink Connector It has the main composite transform (WriteToFirehose), and it expects a list or tuple PCollection element. If the element is a tuple, the tuple\u0026rsquo;s first element is taken. If the element is not of the accepted types, you can apply the GroupIntoBatches or BatchElements transform beforehand. Then, the records of the element are sent into a Firehose delivery stream using the put_record_batch method of the boto3 package. Note that the above batch transforms can also be useful to overcome the API limitation listed below.\nEach PutRecordBatch request supports up to 500 records. Each record in the request can be as large as 1,000 KB (before base64 encoding), up to a limit of 4 MB for the entire request. These limits cannot be changed. The transform also has options that control individual records as well as handle failed records.\njsonify - A flag that indicates whether to convert a record into JSON. Note that a record should be of bytes, bytearray or file-like object, and, if it is not of a supported type (e.g. integer), we can convert it into a Json string by specifying this flag to True. multiline - A flag that indicates whether to add a new line character (\\n) to each record. It is useful to save records into a CSV or Jsonline file. max_trials - The maximum number of trials when there is one or more failed records - it defaults to 3. Note that failed records after all trials are returned, which allows users to determine how to handle them subsequently. append_error - Whether to append error details to failed records. Defaults to True. As mentioned earlier, failed elements are returned by a tagged output where it is named as write-to-firehose-failed-output by default. You can change the name by specifying a different name using the failed_output argument.\nSink Connector Example The example shows how to put records to a Firehose delivery stream that delivers data into an S3 bucket. We first need to create a delivery stream and related resources using the following Python script. The source can be found in the examples folder of the connector repository.\n1# create_resources.py 2import json 3import boto3 4from botocore.exceptions import ClientError 5 6COMMON_NAME = \u0026#34;firehose-pyio-test\u0026#34; 7 8 9def create_destination_bucket(bucket_name): 10 client = boto3.client(\u0026#34;s3\u0026#34;) 11 suffix = client._client_config.region_name 12 client.create_bucket(Bucket=f\u0026#34;{bucket_name}-{suffix}\u0026#34;) 13 14 15def create_firehose_iam_role(role_name): 16 assume_role_policy_document = json.dumps( 17 { 18 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 19 \u0026#34;Statement\u0026#34;: [ 20 { 21 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 22 \u0026#34;Principal\u0026#34;: {\u0026#34;Service\u0026#34;: \u0026#34;firehose.amazonaws.com\u0026#34;}, 23 \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, 24 } 25 ], 26 } 27 ) 28 client = boto3.client(\u0026#34;iam\u0026#34;) 29 try: 30 return client.get_role(RoleName=role_name) 31 except ClientError as error: 32 if error.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;NoSuchEntity\u0026#34;: 33 resp = client.create_role( 34 RoleName=role_name, AssumeRolePolicyDocument=assume_role_policy_document 35 ) 36 client.attach_role_policy( 37 RoleName=role_name, 38 PolicyArn=\u0026#34;arn:aws:iam::aws:policy/AmazonS3FullAccess\u0026#34;, 39 ) 40 return resp 41 42 43def create_delivery_stream(delivery_stream_name, role_arn, bucket_name): 44 client = boto3.client(\u0026#34;firehose\u0026#34;) 45 suffix = client._client_config.region_name 46 try: 47 client.create_delivery_stream( 48 DeliveryStreamName=delivery_stream_name, 49 DeliveryStreamType=\u0026#34;DirectPut\u0026#34;, 50 S3DestinationConfiguration={ 51 \u0026#34;RoleARN\u0026#34;: role_arn, 52 \u0026#34;BucketARN\u0026#34;: f\u0026#34;arn:aws:s3:::{bucket_name}-{suffix}\u0026#34;, 53 \u0026#34;BufferingHints\u0026#34;: {\u0026#34;SizeInMBs\u0026#34;: 1, \u0026#34;IntervalInSeconds\u0026#34;: 0}, 54 }, 55 ) 56 except ClientError as error: 57 if error.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;ResourceInUseException\u0026#34;: 58 pass 59 else: 60 raise error 61 62 63if __name__ == \u0026#34;__main__\u0026#34;: 64 print(\u0026#34;create a destination bucket...\u0026#34;) 65 create_destination_bucket(COMMON_NAME) 66 print(\u0026#34;create an iam role...\u0026#34;) 67 iam_resp = create_firehose_iam_role(COMMON_NAME) 68 print(\u0026#34;create a delivery stream...\u0026#34;) 69 create_delivery_stream(COMMON_NAME, iam_resp[\u0026#34;Role\u0026#34;][\u0026#34;Arn\u0026#34;], COMMON_NAME) The main example script is constructed so that it (1) deletes all existing objects in the S3 bucket, (2) runs the example pipeline and (3) prints contents of the object(s) created by the pipeline.\nThe pipeline begins with creating sample elements where each element is a dictionary that has the id, name and created_at (created date time) attributes. Then, we apply the following two transforms before we apply the main transform (WriteToFirehose).\nDatetimeToStr - It converts the created_at attribute values into string because the Python datetime class cannot be converted into Json by default. BatchElements - It batches the elements into the minimum batch size of 50. It prevents the individual dictionary element from being pushed into the WriteToFirehose transform. In the WriteToFirehose transform, it is configured that individual records are converted into JSON (jsonify=True) as well as a new line character is appended (multiline=True). The former is required because the Python dictionary is not a supported data type while the latter makes the records are saved as JSONLines.\n1# pipeline.py 2import argparse 3import datetime 4import random 5import string 6import logging 7import boto3 8import time 9 10import apache_beam as beam 11from apache_beam.transforms.util import BatchElements 12from apache_beam.options.pipeline_options import PipelineOptions 13from apache_beam.options.pipeline_options import SetupOptions 14 15from firehose_pyio.io import WriteToFirehose 16 17 18def get_all_contents(bucket_name): 19 client = boto3.client(\u0026#34;s3\u0026#34;) 20 bucket_objects = client.list_objects_v2( 21 Bucket=f\u0026#34;{bucket_name}-{client._client_config.region_name}\u0026#34; 22 ) 23 return bucket_objects.get(\u0026#34;Contents\u0026#34;) or [] 24 25 26def delete_all_objects(bucket_name): 27 client = boto3.client(\u0026#34;s3\u0026#34;) 28 contents = get_all_contents(bucket_name) 29 for content in contents: 30 client.delete_object( 31 Bucket=f\u0026#34;{bucket_name}-{client._client_config.region_name}\u0026#34;, 32 Key=content[\u0026#34;Key\u0026#34;], 33 ) 34 35 36def print_bucket_contents(bucket_name): 37 client = boto3.client(\u0026#34;s3\u0026#34;) 38 contents = get_all_contents(bucket_name) 39 for content in contents: 40 resp = client.get_object( 41 Bucket=f\u0026#34;{bucket_name}-{client._client_config.region_name}\u0026#34;, 42 Key=content[\u0026#34;Key\u0026#34;], 43 ) 44 print(f\u0026#34;Key - {content[\u0026#39;Key\u0026#39;]}\u0026#34;) 45 print(resp[\u0026#34;Body\u0026#34;].read().decode()) 46 47 48def create_records(n=100): 49 return [ 50 { 51 \u0026#34;id\u0026#34;: i, 52 \u0026#34;name\u0026#34;: \u0026#34;\u0026#34;.join(random.choices(string.ascii_letters, k=5)).lower(), 53 \u0026#34;created_at\u0026#34;: datetime.datetime.now(), 54 } 55 for i in range(n) 56 ] 57 58 59def convert_ts(record: dict): 60 record[\u0026#34;created_at\u0026#34;] = record[\u0026#34;created_at\u0026#34;].isoformat(timespec=\u0026#34;milliseconds\u0026#34;) 61 return record 62 63 64def mask_secrets(d: dict): 65 return {k: (v if k.find(\u0026#34;aws\u0026#34;) \u0026lt; 0 else \u0026#34;x\u0026#34; * len(v)) for k, v in d.items()} 66 67 68def run(argv=None, save_main_session=True): 69 parser = argparse.ArgumentParser(description=\u0026#34;Beam pipeline arguments\u0026#34;) 70 parser.add_argument( 71 \u0026#34;--stream_name\u0026#34;, 72 default=\u0026#34;firehose-pyio-test\u0026#34;, 73 type=str, 74 help=\u0026#34;Delivery stream name\u0026#34;, 75 ) 76 parser.add_argument( 77 \u0026#34;--num_records\u0026#34;, default=\u0026#34;100\u0026#34;, type=int, help=\u0026#34;Number of records\u0026#34; 78 ) 79 known_args, pipeline_args = parser.parse_known_args(argv) 80 81 pipeline_options = PipelineOptions(pipeline_args) 82 pipeline_options.view_as(SetupOptions).save_main_session = save_main_session 83 print(f\u0026#34;known_args - {known_args}\u0026#34;) 84 print(f\u0026#34;pipeline options - {mask_secrets(pipeline_options.display_data())}\u0026#34;) 85 86 with beam.Pipeline(options=pipeline_options) as p: 87 ( 88 p 89 | \u0026#34;CreateElements\u0026#34; \u0026gt;\u0026gt; beam.Create(create_records(known_args.num_records)) 90 | \u0026#34;DatetimeToStr\u0026#34; \u0026gt;\u0026gt; beam.Map(convert_ts) 91 | \u0026#34;BatchElements\u0026#34; \u0026gt;\u0026gt; BatchElements(min_batch_size=50) 92 | \u0026#34;WriteToFirehose\u0026#34; 93 \u0026gt;\u0026gt; WriteToFirehose( 94 delivery_stream_name=known_args.stream_name, 95 jsonify=True, 96 multiline=True, 97 max_trials=3, 98 ) 99 ) 100 101 logging.getLogger().setLevel(logging.WARN) 102 logging.info(\u0026#34;Building pipeline ...\u0026#34;) 103 104 105if __name__ == \u0026#34;__main__\u0026#34;: 106 BUCKET_NAME = \u0026#34;firehose-pyio-test\u0026#34; 107 print(\u0026#34;\u0026gt;\u0026gt; delete existing objects...\u0026#34;) 108 delete_all_objects(BUCKET_NAME) 109 print(\u0026#34;\u0026gt;\u0026gt; start pipeline...\u0026#34;) 110 run() 111 time.sleep(1) 112 print(\u0026#34;\u0026gt;\u0026gt; print bucket contents...\u0026#34;) 113 print_bucket_contents(BUCKET_NAME) We can run the pipeline on any runner that supports the Python SDK. Below shows an example of running the example pipeline on the Apache Flink Runner. Note that AWS related values (e.g. aws_access_key_id) can be specified as pipeline arguments because the package has a dedicated pipeline option (FirehoseOptions) that parses them. Once the pipeline runs successfully, the script continues to read the contents of the file object(s) that are created by the connector.\n1python examples/pipeline.py \\ 2 --runner=FlinkRunner \\ 3 --parallelism=1 \\ 4 --aws_access_key_id=$AWS_ACCESS_KEY_ID \\ 5 --aws_secret_access_key=$AWS_SECRET_ACCESS_KEY \\ 6 --region_name=$AWS_DEFAULT_REGION 1\u0026gt;\u0026gt; start pipeline... 2known_args - Namespace(stream_name=\u0026#39;firehose-pyio-test\u0026#39;, num_records=100) 3pipeline options - {\u0026#39;runner\u0026#39;: \u0026#39;FlinkRunner\u0026#39;, \u0026#39;save_main_session\u0026#39;: True, \u0026#39;parallelism\u0026#39;: 1, \u0026#39;aws_access_key_id\u0026#39;: \u0026#39;xxxxxxxxxxxxxxxxxxxx\u0026#39;, \u0026#39;aws_secret_access_key\u0026#39;: \u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#39;, \u0026#39;region_name\u0026#39;: \u0026#39;us-east-1\u0026#39;} 4\u0026gt;\u0026gt; print bucket contents... 5Key - 2024/07/21/01/firehose-pyio-test-1-2024-07-21-01-49-51-5fd0fc8d-b656-4f7b-9bc6-5039975d941f 6{\u0026#34;id\u0026#34;: 50, \u0026#34;name\u0026#34;: \u0026#34;amlis\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 7{\u0026#34;id\u0026#34;: 51, \u0026#34;name\u0026#34;: \u0026#34;bqvhr\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 8{\u0026#34;id\u0026#34;: 52, \u0026#34;name\u0026#34;: \u0026#34;stsbv\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 9{\u0026#34;id\u0026#34;: 53, \u0026#34;name\u0026#34;: \u0026#34;rttjg\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 10{\u0026#34;id\u0026#34;: 54, \u0026#34;name\u0026#34;: \u0026#34;avozb\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 11{\u0026#34;id\u0026#34;: 55, \u0026#34;name\u0026#34;: \u0026#34;fyesu\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 12{\u0026#34;id\u0026#34;: 56, \u0026#34;name\u0026#34;: \u0026#34;pvxlw\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 13{\u0026#34;id\u0026#34;: 57, \u0026#34;name\u0026#34;: \u0026#34;qyjlo\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 14{\u0026#34;id\u0026#34;: 58, \u0026#34;name\u0026#34;: \u0026#34;smhns\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-07-21T11:49:32.536\u0026#34;} 15... Note that the following warning messages are printed if it installs the grpcio (1.65.x) package (see this GitHub issue). You can downgrade the package version to avoid those messages (eg pip install grpcio==1.64.1).\n1WARNING: All log messages before absl::InitializeLog() is called are written to STDERR 2I0000 00:00:1721526572.886302 78332 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache 3I0000 00:00:1721526573.143589 78363 subchannel.cc:806] subchannel 0x7f4010001890 {address=ipv6:%5B::1%5D:58713, args={grpc.client_channel_factory=0x2ae79b0, grpc.default_authority=localhost:58713, grpc.internal.channel_credentials=0x297dda0, grpc.internal.client_channel_call_destination=0x7f407f3b23d0, grpc.internal.event_engine=0x7f4010013870, grpc.internal.security_connector=0x7f40100140e0, grpc.internal.subchannel_pool=0x21d3d00, grpc.max_receive_message_length=-1, grpc.max_send_message_length=-1, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x2a99310, grpc.server_uri=dns:///localhost:58713}}: connect failed (UNKNOWN:Failed to connect to remote host: connect: Connection refused (111) {created_time:\u0026#34;2024-07-21T11:49:33.143192444+10:00\u0026#34;}), backing off for 1000 ms More Sink Connector Examples More usage examples can be found in the unit testing cases. Some of them are covered here.\nOnly the list or tuple types are supported PCollection elements. In the following example, individual string elements are applied in the WriteToFirehose, and it raises the TypeError. 1def test_write_to_firehose_with_unsupported_types(self): 2 # only the list type is supported! 3 with self.assertRaises(FirehoseClientError): 4 with TestPipeline(options=self.pipeline_opts) as p: 5 ( 6 p 7 | beam.Create([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]) 8 | WriteToFirehose( 9 delivery_stream_name=self.delivery_stream_name, 10 jsonify=False, 11 multiline=False, 12 ) 13 ) Jsonify the element if it is not of the bytes, bytearray or file-like object. In this example, the second element is a list of integers, and it should be converted into JSON (jsonify=True). Or we can convert it into string manually. 1def test_write_to_firehose_with_list_elements(self): 2 with TestPipeline(options=self.pipeline_opts) as p: 3 output = ( 4 p 5 | beam.Create([[\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;], [1, 2, 3, 4]]) 6 | WriteToFirehose( 7 delivery_stream_name=self.delivery_stream_name, 8 jsonify=True, 9 multiline=False, 10 ) 11 ) 12 assert_that(output[None], equal_to([])) 13 14 bucket_contents = collect_bucket_contents(self.s3_client, self.bucket_name) 15 self.assertSetEqual( 16 set(bucket_contents), set([\u0026#39;\u0026#34;one\u0026#34;\u0026#34;two\u0026#34;\u0026#34;three\u0026#34;\u0026#34;four\u0026#34;\u0026#39;, \u0026#34;1234\u0026#34;]) 17 ) If an element is a tuple, its first element is applied to the WriteToFirehose transform. 1def test_write_to_firehose_with_tuple_elements(self): 2 with TestPipeline(options=self.pipeline_opts) as p: 3 output = ( 4 p 5 | beam.Create([(1, [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]), (2, [1, 2, 3, 4])]) 6 | WriteToFirehose( 7 delivery_stream_name=self.delivery_stream_name, 8 jsonify=True, 9 multiline=False, 10 ) 11 ) 12 assert_that(output[None], equal_to([])) 13 14 bucket_contents = collect_bucket_contents(self.s3_client, self.bucket_name) 15 self.assertSetEqual( 16 set(bucket_contents), set([\u0026#39;\u0026#34;one\u0026#34;\u0026#34;two\u0026#34;\u0026#34;three\u0026#34;\u0026#34;four\u0026#34;\u0026#39;, \u0026#34;1234\u0026#34;]) 17 ) We can batch an element if it is not of the supported types. Note that, a new line character (\\n) is appended to each record, and it is particularly useful for saving a CSV or JSONLines file to S3. 1def test_write_to_firehose_with_list_multilining(self): 2 with TestPipeline(options=self.pipeline_opts) as p: 3 output = ( 4 p 5 | beam.Create([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]) 6 | BatchElements(min_batch_size=2, max_batch_size=2) 7 | WriteToFirehose( 8 delivery_stream_name=self.delivery_stream_name, 9 jsonify=False, 10 multiline=True, 11 ) 12 ) 13 assert_that(output[None], equal_to([])) 14 15 bucket_contents = collect_bucket_contents(self.s3_client, self.bucket_name) 16 self.assertSetEqual(set(bucket_contents), set([\u0026#34;one\\ntwo\\n\u0026#34;, \u0026#34;three\\nfour\\n\u0026#34;])) 17 18def test_write_to_firehose_with_tuple_multilining(self): 19 with TestPipeline(options=self.pipeline_opts) as p: 20 output = ( 21 p 22 | beam.Create([(1, \u0026#34;one\u0026#34;), (2, \u0026#34;three\u0026#34;), (1, \u0026#34;two\u0026#34;), (2, \u0026#34;four\u0026#34;)]) 23 | GroupIntoBatches(batch_size=2) 24 | WriteToFirehose( 25 delivery_stream_name=self.delivery_stream_name, 26 jsonify=False, 27 multiline=True, 28 ) 29 ) 30 assert_that(output[None], equal_to([])) 31 32 bucket_contents = collect_bucket_contents(self.s3_client, self.bucket_name) 33 self.assertSetEqual(set(bucket_contents), set([\u0026#34;one\\ntwo\\n\u0026#34;, \u0026#34;three\\nfour\\n\u0026#34;])) Failed records after all trials are returned by a tagged output. We can configure the number of trials (max_trials) and whether to append error details (append_error). 1class TestRetryLogic(unittest.TestCase): 2 # default failed output name 3 failed_output = \u0026#34;write-to-firehose-failed-output\u0026#34; 4 5 def test_write_to_firehose_retry_with_no_failed_element(self): 6 with TestPipeline() as p: 7 output = ( 8 p 9 | beam.Create([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]) 10 | BatchElements(min_batch_size=4) 11 | WriteToFirehose( 12 delivery_stream_name=\u0026#34;non-existing-delivery-stream\u0026#34;, 13 jsonify=False, 14 multiline=False, 15 max_trials=3, 16 append_error=False, 17 fake_config={\u0026#34;num_success\u0026#34;: 2}, 18 ) 19 ) 20 assert_that(output[self.failed_output], equal_to([])) 21 22 def test_write_to_firehose_retry_failed_element_without_appending_error(self): 23 with TestPipeline() as p: 24 output = ( 25 p 26 | beam.Create([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]) 27 | BatchElements(min_batch_size=4) 28 | WriteToFirehose( 29 delivery_stream_name=\u0026#34;non-existing-delivery-stream\u0026#34;, 30 jsonify=False, 31 multiline=False, 32 max_trials=3, 33 append_error=False, 34 fake_config={\u0026#34;num_success\u0026#34;: 1}, 35 ) 36 ) 37 assert_that(output[self.failed_output], equal_to([\u0026#34;four\u0026#34;])) 38 39 def test_write_to_firehose_retry_failed_element_with_appending_error(self): 40 with TestPipeline() as p: 41 output = ( 42 p 43 | beam.Create([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]) 44 | BatchElements(min_batch_size=4) 45 | WriteToFirehose( 46 delivery_stream_name=\u0026#34;non-existing-delivery-stream\u0026#34;, 47 jsonify=False, 48 multiline=False, 49 max_trials=3, 50 append_error=True, 51 fake_config={\u0026#34;num_success\u0026#34;: 1}, 52 ) 53 ) 54 assert_that( 55 output[self.failed_output], 56 equal_to( 57 [(\u0026#34;four\u0026#34;, {\u0026#34;ErrorCode\u0026#34;: \u0026#34;Error\u0026#34;, \u0026#34;ErrorMessage\u0026#34;: \u0026#34;This error\u0026#34;})] 58 ), 59 ) ","date":"July 18, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/firehose-pyio-intro/","series":[],"smallImg":"","tags":[],"timestamp":1721260800,"title":"Introduction to Firehose PyIO Sink Connector"},{"authors":[{"title":"Jaehyeon Kim","url":"/authors/jaehyeonkim/"}],"categories":[{"title":"Announcement","url":"/categories/announcement/"}],"content":"Apache Beam is an open source unified programming model to define and execute data processing pipelines, including ETL, batch and stream processing. We consider it has a huge potential to improve traditional development patterns in both transactional and analytical processing of data. Specifically it can be applied to event-driven applications, data pipelines and streaming analytics.\nEmploying dataflow programming, Beam supports a range of I/O connectors, but we find some gaps in the existing connectors especially in relation to the Python SDK. It fueled us to start the Apache Beam Python I/O Connectors project.\nAs long time AWS users, we see connectors for some key AWS services are missing or are not available in the Python SDK. Those services cover Firehose, SQS, SNS, DynamoDB and EventBridge, and we have started developing connectors for them.\nIn data engineering projects, OLTP/OLAP systems and open table formats (Apache Iceberg, Apache Hudi and Delta Lake) are key data sources and destinations (sinks). Python-native connectors can make it simpler to develop data pipelines that deals with those data storage systems, and we plan to develop relevant connectors by integrating with the daft package.\nWe keep looking into adding new connectors. If you have a new idea, please add a comment to this discussion or check the project repository for updates on new connectors.\n","date":"July 4, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/blog/2024/beam-pyio-intro/","series":[],"smallImg":"","tags":[],"timestamp":1720051200,"title":"Apache Beam Python I/O Connectors"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"}]
